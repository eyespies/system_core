---
driver:
  name: ec2
  require_chef_omnibus: true
  aws_ssh_key_id: "<%= ENV.has_key?('AWS_SSH_KEY_ID') ? ENV['AWS_SSH_KEY_ID'] : 'bitbucket-pipelines' %>"
  security_group_ids: ["<%= ENV['DRIVER_SECURITY_GROUP_IDS'] %>"]
  region: "<%= ENV['AWS_DEFAULT_REGION'] %>"
  # This is needed only if the subnet_filter element is not used.
  # availability_zone: "<%= ENV['DRIVER_AVAILABILITY_ZONE'] %>"
  # subnet_id: "<%= ENV['DRIVER_SUBNET_ID'] %>"
  subnet_filter:
    tag: "ChefTests"
    value: "true"
  iam_profile_name: ChefTestKitchen
  # Spot price is required for kitchen-ec2 < 3.0.0; later versions allow setting this to 'ondemand'
  spot_price: "ondemand"
  # The documentation for kitchen-ec2 states that an array of options can be specified, however the AWS SDK call fails with the error:
  #   Message: Failed to complete #create action: [expected params[:launch_specification][:instance_type] to be a
  #   String, got value ["c5dlarge", "r5d.large", "m5d.large", "m5d.xlarge"] (class: Array) instead.]
  # instance_type: "<%=  ENV.has_key?('DRIVER_INSTANCE_TYPE') ? ENV['DRIVER_INSTANCE_TYPE'] : 'm5d.large' %>"
  # Specifying an array is only available with kitchen-ec2 v 3.x
  instance_type:
    - c5d.large
    - m5d.large
    - r5d.large
    - c5d.xlarge
    - m5d.xlarge
  associate_public_ip: true
  interface: dns
  # The default Oracle EC2 instance includes 'yum update -y' in the runcmd section; this causes problems when running Chef
  # because the updating takes long enough that it is still running when Chef runs. This in turn causes any 'package'
  # resources in Chef to fail because the RPM DB / Yum is already locked. The below is the Cloud Init taken directly from
  # an Oracle EC2 instance.
  user_data: |
    #cloud-config
    users:
    - default

    disable_root: 1
    ssh_pwauth:   0

    mount_default_fields: [~, ~, 'auto', 'defaults,nofail', '0', '2']
    resize_rootfs_tmp: /dev
    ssh_deletekeys:   0
    ssh_genkeytypes:  ~
    syslog_fix_perms: ~

    cloud_init_modules:
    - migrator
    - bootcmd
    - write-files
    - growpart
    - resizefs
    - set_hostname
    - update_hostname
    - update_etc_hosts
    - rsyslog
    - users-groups
    - ssh

    cloud_config_modules:
    - mounts
    - locale
    - set-passwords
    - yum-add-repo
    - package-update-upgrade-install
    - timezone
    - puppet
    - chef
    - salt-minion
    - mcollective
    - disable-ec2-metadata
    - runcmd

    cloud_final_modules:
    - rightscale_userdata
    - scripts-per-once
    - scripts-per-boot
    - scripts-per-instance
    - scripts-user
    - ssh-authkey-fingerprints
    - keys-to-console
    - phone-home
    - final-message

    system_info:
      default_user:
        name: ec2-user
        lock_passwd: true
        gecos: Cloud User
        groups: [wheel, adm, systemd-journal]
        sudo: ["ALL=(ALL) NOPASSWD:ALL"]
        shell: /bin/bash
      distro: rhel
      paths:
        cloud_dir: /var/lib/cloud
        templates_dir: /etc/cloud/templates
      ssh_svcname: sshd

    runcmd:
      ["test -e /usr/bin/ol_yum_configure.sh && sh /usr/bin/ol_yum_configure.sh"]

    # vim:syntax=yaml


transport:
  ssh_key: "<%= ENV.has_key?('SSH_PRIVATE_KEY') ? ENV['SSH_PRIVATE_KEY'] : '~/.ssh/id_rsa' %>"
  connection_timeout: 10
  connection_retries: 5
  # username: ubuntu

provisioner:
  product_name: chef
  product_version: "<%= ENV.has_key?('CHEF_VERSION') ? ENV['CHEF_VERSION'] : 17 %>"
  # You may wish to disable always updating cookbooks in CI or other testing environments.
  # For example:
  #   always_update_cookbooks: <%= !ENV['CI'] %>
  always_update_cookbooks: true
  chef_license: accept
  # This sets the logging level for Chef inside of the instance. To set the log level for Test
  # Kitchen itself, use '-l debug' at the command line when running Kitchen.
  # log_level: debug

verifier:
  name: inspec

platforms:
  - name: ubuntu-18.04
    driver:
      image_search:
        owner-id: "099720109477"
        name: "ubuntu/images/hvm-ssd/ubuntu-bionic-18.04"
    transport:
      username: ubuntu
  - name: ubuntu-20.04
    driver:
      image_search:
        owner-id: "099720109477"
        name: "ubuntu/images/hvm-ssd/ubuntu-focal-20.04"
    transport:
      username: ubuntu
  - name: centos-7
    driver:
      # To see a list of CentOS images, use the command below
      # `aws ec2 describe-images --owners 679593333241 --filters 'Name=is-public,Values=true' --filters 'Name=virtualization-type,Values=hvm' | jq -r '.Images |=sort_by(.CreationDate) | .Images[] | select(.Name | test("^CentOS Linux 7 x86_64")) | { Desc: .Description, Name: .Name, ImageId: .ImageId, Created: .CreationDate}'`
      image_search:
        owner-id: "679593333241"
        name: "CentOS Stream 7 x86_64"
    transport:
      username: centos
  - name: centos-8
    driver:
      # To see a list of CentOS images, use the command below
      # `aws ec2 describe-images --owners 125523088429 --filters 'Name=is-public,Values=true' --filters 'Name=virtualization-type,Values=hvm' | jq -r '.Images |=sort_by(.CreationDate) | .Images[] | select(.Name | test("^CentOS Stream 8 x86_64")) | { Desc: .Description, Name: .Name, ImageId: .ImageId, Created: .CreationDate}'`
      image_search:
        owner-id: "125523088429"
        name: "CentOS Stream 8 x86_64"
    transport:
      username: centos
  - name: oracle-7
    driver:
      # To find the image ID for Oracle instances, use the command below, making sure to replace the 'OL7.6' with the correct name prefix
      # `aws ec2 describe-images --owners 131827586825 --filters 'Name=is-public,Values=true' --filters 'Name=virtualization-type,Values=hvm' | jq -r '.Images |=sort_by(.CreationDate) | .Images[] | select(.Name | test("^OL7")) | { Desc: .Description, Name: .Name, ImageId: .ImageId, Created: .CreationDate}'`
      image_id: "<%= ENV['PLATFORMS_ORACLE7_DRIVER_IMAGEID'] %>"
      block_device_mappings:
        - device_name: /dev/sda1
          ebs:
            volume_size: 32
            delete_on_termination: true
    transport:
      username: ec2-user
  - name: oracle-8
    driver:
      # To find the image ID for Oracle instances, use the command below, making sure to replace the 'OL7.6' with the correct name prefix
      # `aws ec2 describe-images --owners 131827586825 --filters 'Name=is-public,Values=true' --filters 'Name=virtualization-type,Values=hvm' | jq -r '.Images |=sort_by(.CreationDate) | .Images[] | select(.Name | test("^OL8")) | { Desc: .Description, Name: .Name, ImageId: .ImageId, Created: .CreationDate}'`
      image_id: "<%= ENV['PLATFORMS_ORACLE8_DRIVER_IMAGEID'] %>"
      # This doesn't work well because there is no 'standard_platform/oracle.rb' configuration file for the driver.
      # image_search:
      #   architecture: "x86_64"
      #   is-public: true
      #   name: "OL7.6-X86_64-HVM-*"
      #   owner-id: "131827586825"
      #   virtualization-type: "hvm"
      block_device_mappings:
        - device_name: /dev/sda1
          ebs:
            volume_size: 32
            delete_on_termination: true
    transport:
      username: ec2-user

suites:
  - name: base
    data_bags_path: "../../data_bags"
    run_list:
      - recipe[system_core::default]
      # This seems to be automatically installed by RHEL / CentOS / Oracle,
      # but not by Debian, which causes the Debian tests to fail because they
      # are looking for auditd
      - recipe[system_core::auditd]
      - recipe[system_core::repos]
      - recipe[system_core::awscli]
      - recipe[system_core::bash]
      - recipe[system_core::postfix]
      - recipe[system_core::ntp]
      - recipe[system_core::selinux]
      - recipe[system_core::ssh]
      - recipe[system_core::logrotate]
      - recipe[system_core::user_ssh]
    attributes:
      system_core:
        ssh:
          authorized_keys:
            root:
              public_key: "AAAAB3NzaC1yc2EAAAADAQABAAABAQDJlvaL0I0HWNE/RblFscFWhjXDwX6UaMBLtG5YdbHHSc1QQO+W+kV15q3T7WnED6In+aK423OzMTk/0/UZrchlxa2KCRNSnRrqViTZZ1XUXwEXqCnBQ9O1El93AAaE73suB9kYfeO105D5AgTTmf41HDc4YAxZtoAOt2KdI2GF7+7IfheI54aWSldmQesfqNloY+ivYIOhyEIwXuO9RS2BEbrFoxuVfOcz62AGcFz07EsALWGNzr4ngT6pe8vCbV5s/f0cDk5z9XZ4Wk2uQI7NQuLkSOmokU3QqZhOYJUjjTdq8VrjARWdF7K5N0/LQ/Wyx6Tgy+XRavnmj/SMhaKd"
              user: 'root'
          user_config:
            root:
              hosts:
                \*:
                  SendEnv: "LANG LC_*"
                  ForwardAgent: "yes"
                  UserKnownHostsFile: "/dev/null"
                  StrictHostKeyChecking: "no"
              keys:
                id_rsa:
                  owner: root
                  ssh_key_bucket: "<%= ENV['SSH_KEY_BUCKET'] %>"
                  ssh_key_path: "<%= ENV.key?('ROOT_SSH_KEY_PATH') ? ENV['ROOT_SSH_KEY_PATH'] : 'ssh-keys/root' %>"
        aws:
          profiles:
            default:
              output_format: "json"
              region: "<%= ENV.key('AWS_REGION') ? ENV['AWS_REGION'] : 'us-east-1' %>"
              access_key: "<%= ENV['AWS_ACCESS_KEY_ID'] %>"
              secret_key: "<%= ENV['AWS_SECRET_ACCESS_KEY'] %>"
  - name: logging
    data_bags_path: "../../data_bags"
    run_list:
      - recipe[system_core::default]
      - recipe[system_core::rsyslog]
      - recipe[system_core::logrotate]
      - recipe[system_core::papertrail]
      - recipe[system_core::papertrail_remote]
      - recipe[logrotate::default]
    verifier:
      attributes:
        papertrail_host: '@@log.example.com:11111'
    attributes:
      system_core:
        papertrail:
          enabled: true
          remote_host: '@@log.example.com:11111'
          remote_syslog2:
            files:
              /some/file/without-tags:
              /some/file/with-tag: 'test'
              /some/second/file/with-tag: 'test'
              /some/third/file/with-tag: 'test'
              /some/second/file/without-tags:
  - name: ruby
    data_bags_path: "../../data_bags"
    run_list:
      - recipe[system_core::default]
      - recipe[system_core::repos]
      - recipe[system_core::rbenv]
  - name: altdomain
    data_bags_path: "../../data_bags"
    run_list:
      - recipe[system_core::default]
      - recipe[system_core::repos]
    verifier:
      attributes:
        default_domain: linux.alt.org
    attributes:
      system_core:
        aws:
          profiles:
            default:
              output_format: "json"
              region: "<%= ENV.key('AWS_REGION') ? ENV['AWS_REGION'] : 'us-east-1' %>"
              access_key: "<%= ENV['AWS_ACCESS_KEY_ID'] %>"
              secret_key: "<%= ENV['AWS_SECRET_ACCESS_KEY'] %>"
        domain: "linux.alt.org"
